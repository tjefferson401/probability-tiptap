
% rebase('templates/chapter.html', title="Probability of and")
 

<center><h1>Probability of <b>and</b></h1></center>
<hr/>

<p>The probability of the <b><i>and</i></b> of two events, say $E$ and $F$, written $\p(E \and F)$, is the probability of both events happening. You might see equivalent notations $\p(EF)$, $\p(E ∩ F)$ and $\p(E,F)$ to mean the probability of and. How you calculate the probability of event $E$ and event $F$ happening
depends on whether or not the events are "independent". In the same way that mutual exclusion makes it easy to calculate the probability of the <b><i>or</i></b> of events, independence is a property that makes it easy to calculate the probability of the <b><i>and</i></b> of events.</p>     

<h2><b>And</b> with Independent Events</h2>

<p>If events are <a href="{{pathToLang}}part1/independence"><b></i>independent</i></b></a> then calculating the probability of <b><i>and</i></b> becomes simple multiplication:

<p>
	<div class="bordered">
		<b><i>Definition</i></b>: Probability of <b><i>and</i></b> for independent events.<br/>

		<p>If two events: $E$, $F$ are independent then the probability of $E$ <b><i>and</i></b> $F$ occurring is:
			$$
			\p(E \and F) = \p(E) \cdot \p(F)
			$$
		</p> 
		<p>This property applies regardless of how the probabilities of $E$ and $F$ were calculated and
whether or not the events are mutually exclusive. </p>

<p> The independence principle extends to more than two
events. For $n$ events $E_1, E_2, \dots E_n$ that are <b><i>mutually</i></b> independent of one another -- the independence equation also holds for all subsets of the events.
$$
\p(E_1 \and E_2 \and \dots \and E_n) = \prod_{i=1}^n \p(E_i)
$$
</p>
<p></p>
	</div>
</p>

<p >
	We can prove this equation by combining the definition of conditional probability and the definition of independence.
	<div class="purpleBox">
	<p><b><i>Proof</i></b>: If $E$ is independent of $F$ then $\p(E \and F) = \p(E) \cdot \p(F)$</p>
	$$
	\begin{align}
	\p(E|F) &= \frac{\p(E \and F)}{\p(F)} && \text{Definition of }
	\href{ {{pathToLang}}part1/cond_prob/}{\text{conditional probability}} 
	\\
	\p(E) &=  \frac{\p(E \and F)}{\p(F)} && \text{Definition of }
	\href{ {{pathToLang}}part1/independence/}{\text{independence}} \\
	\p(E \and F) &= \p(E) \cdot \p(F) && \text{Rearranging terms}
	\end{align}
	$$
</div>
	</p>

<p>See the chapter on <a href="{{pathToLang}}part1/independence">independence</a> to learn about when you can assume that two events are independent</p>

<h2><b>And</b> with Dependent Events</h2>

<p>Events which are not independent are called <b><i>dependent</i></b> events. How can you calculate the probability of the <b>and</b> of dependent events? If your events are mutually exclusive you might be able to use a technique called DeMorgan's law, which we cover in a later chapter. For the probability of and in dependent events there is a direct formula called the chain rule which can be directly derived from the definition of conditional probability:

	<p>
	<div class="bordered">

<b><i>Definition</i></b>: The chain rule.<br/>

<p>The formula in the definition of conditional probability can be re-arranged to derive a general way of calculating the probability of the <b><i>and</i></b> of any two events:
$$
\p(E \and F) = \p(E | F) \cdot \p(F)
$$
</p>
<p>Of course there is nothing special about $E$ that says it should go first. Equivalently:
	$$
	\p(E \and F) = \p(F \and E) =  \p(F | E) \cdot \p(E)
	$$
</p>

<p>We call this formula the "chain rule." Intuitively it states that the probability of observing events $E$ <b><i>and</i></b> $F$ is the
probability of observing $F$, multiplied by the probability of observing $E$, given that you have observed $F$.
It generalizes to more than two events:
$$
\begin{align}
\p(E_1 \and E_2 \and \dots \and E_n) = &\p(E_1) \cdot \p(E_2|E_1) \cdot \p(E_3 |E_1 \and E_2) \dots  \\  &\p(E_n|E_1 \dots E_{n−1})
\end{align}
$$
	</div>
	</p>


</p>